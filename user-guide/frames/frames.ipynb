{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e02afb",
   "metadata": {},
   "source": [
    "### Frames\n",
    "A frames object is literally a ``xr.DataArray`` object with certain properties and an expected structure with\n",
    "coordinates and variables. In this section, we show what you can do with such an object. \n",
    "\n",
    "As shown in the videos section, such an object can be directly derived from a video object using the method ``video.get_frames``. An example of the ``xr.DataArray`` structure returned is shown below, with an ``method=\"rgb\"`` setting to get a color image back (default is grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4687383e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:38.870131Z",
     "iopub.status.busy": "2024-09-09T07:53:38.869709Z",
     "iopub.status.idle": "2024-09-09T07:53:40.643794Z",
     "shell.execute_reply": "2024-09-09T07:53:40.643239Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'fixed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# point to a file containing the camera configuration\u001b[39;00m\n\u001b[1;32m      6\u001b[0m cam_config \u001b[38;5;241m=\u001b[39m pyorc\u001b[38;5;241m.\u001b[39mload_camera_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../examples/ngwerere/ngwerere.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[43mpyorc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcam_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m125\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstabilize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfixed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m da_frames \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mget_frames(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m da_frames\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/pyorc-dev/lib/python3.10/site-packages/pyorc/api/video.py:117\u001b[0m, in \u001b[0;36mVideo.__init__\u001b[0;34m(self, fn, camera_config, h_a, start_frame, end_frame, freq, stabilize, lazy, rotation, fps)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# explicitly open file for reading\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstabilize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# set a gridded mask based on the roi points\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_mask_from_exterior\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstabilize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# set end and start frame\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT))\n",
      "File \u001b[0;32m/usr/share/miniconda/envs/pyorc-dev/lib/python3.10/site-packages/pyorc/api/video.py:550\u001b[0m, in \u001b[0;36mVideo.set_mask_from_exterior\u001b[0;34m(self, exterior)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_mask_from_exterior\u001b[39m(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    534\u001b[0m         exterior: List[List]\n\u001b[1;32m    535\u001b[0m ):\n\u001b[1;32m    536\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m    Prepare a mask grid with 255 outside of the stabilization polygon and 0 inside\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     mask_coords \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexterior\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth), np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    552\u001b[0m     mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfillPoly(mask, [mask_coords], \u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'fixed'"
     ]
    }
   ],
   "source": [
    "import pyorc\n",
    "import numpy as np\n",
    "# set a video filename below, change to your own local file location\n",
    "video_file = \"../../../examples/ngwerere/ngwerere_20191103.mp4\"\n",
    "# point to a file containing the camera configuration\n",
    "cam_config = pyorc.load_camera_config(\"../../../examples/ngwerere/ngwerere.json\")\n",
    "video = pyorc.Video(\n",
    "    video_file,\n",
    "    camera_config=cam_config,\n",
    "    start_frame=0,\n",
    "    end_frame=125,\n",
    "    stabilize=\"fixed\"\n",
    ")\n",
    "da_frames = video.get_frames(method=\"rgb\")\n",
    "da_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db853d",
   "metadata": {},
   "source": [
    "It can be seen that this ``xr.DataArray`` contains a data blob with all the frames (i.e. ``time: 125`` meaning we have 125 frames at our disposal for the further analysis) and a ``x`` and ``y`` coordinate system, which simply is a column and row count for the pixels in the image objective. We are looking an HD video here, and so this column and row count is 1920x1080. We also have a dimension ``rgb`` because we hold 3 channels for each image. If you would call ``get_frames`` without any arguments, you would not get this. A few things are noteworthy to further explain about some of the coordinates and attributes.\n",
    "\n",
    "- ``time``: contains an axis which measures the time since the first frame in seconds. It is based on the frames per second property of the video.\n",
    "- ``xp`` and ``yp`` are very similar to ``x`` and ``y``, but they contain a mesh of row, column coordinates, instead of only a 1-dimensional axis. This is required for some of the methods that can be applied on a frames ``xr.DataArray``.\n",
    "- ``camera_config``: crucial for a frames ``xr.DataArray`` is to have understanding of the camera's properties and perspective and relationship with geography. This is all stored in your camera configuration, hence, this camera configuration is also provided to the ``xr.DataArray`` storing your frames. It will be used for further processing steps, in particular reprojection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd52fd",
   "metadata": {},
   "source": [
    "#### Methods for frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029011f",
   "metadata": {},
   "source": [
    "Any manipulation to a frames object that is really particular to the fact you are looking at a frames ``xr.DataArray`` as opposed to any other ``xr.DataArray`` can be found behind a subclass called ``frames``. So to call such a functionality, e.g. the plotting method, you basically call ``da_frames.frames.plot``. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395d12fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.646040Z",
     "iopub.status.busy": "2024-09-09T07:53:40.645868Z",
     "iopub.status.idle": "2024-09-09T07:53:40.658047Z",
     "shell.execute_reply": "2024-09-09T07:53:40.657473Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'da_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mda_frames\u001b[49m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'da_frames' is not defined"
     ]
    }
   ],
   "source": [
    "da_frames.frames.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595adadd",
   "metadata": {},
   "source": [
    "That gave an error. You can see that the API gives specific feedback that the ``.frames.plot`` method does not work on a frames array with several time steps. This is simply because you can only plot one step at the time. You can select a step, with normal ``xarray`` methods, which are very similar to ``numpy``, and then try it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3ccd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.660331Z",
     "iopub.status.busy": "2024-09-09T07:53:40.659909Z",
     "iopub.status.idle": "2024-09-09T07:53:40.672011Z",
     "shell.execute_reply": "2024-09-09T07:53:40.671476Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'da_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# only plot time index 0\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mda_frames\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'da_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# only plot time index 0\n",
    "da_frames[0].frames.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf11863",
   "metadata": {},
   "source": [
    "#### Improving contrast\n",
    "There are a number of ways to improve contrast of moving features. These so far include the following methods:\n",
    "\n",
    "- ``frames.normalization``: removes the average of a number of frames in time. This then yields a better contrast of moving things, compared to static things. This is particularly useful to remove visuals of the bottom, when you have very transparent water. You can set the amount of frames used for averaging.\n",
    "- ``frames.edge_detect``: enhances the visibility of strong spatial gradients in the frame, by applying two kernel filters on the individual frames with varying window sizes, and returning the difference between the filtered images.\n",
    "\n",
    "These operations apply the filter on all frames, and return the result again as a frames ``xr.DataArray``, but then with the filters applied. They can only be applied on single channel (i.e. greyscale) images.\n",
    "\n",
    "We show a result of applying both filters on the first frame below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08c1aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.674206Z",
     "iopub.status.busy": "2024-09-09T07:53:40.673893Z",
     "iopub.status.idle": "2024-09-09T07:53:40.686752Z",
     "shell.execute_reply": "2024-09-09T07:53:40.686213Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m da_grey \u001b[38;5;241m=\u001b[39m \u001b[43mvideo\u001b[49m\u001b[38;5;241m.\u001b[39mget_frames()\n\u001b[1;32m      2\u001b[0m da_norm \u001b[38;5;241m=\u001b[39m da_grey\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mnormalize()\n\u001b[1;32m      3\u001b[0m da_edge \u001b[38;5;241m=\u001b[39m da_norm\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39medge_detect(wdw_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, wdw_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "da_grey = video.get_frames()\n",
    "da_norm = da_grey.frames.normalize()\n",
    "da_edge = da_norm.frames.edge_detect(wdw_1=6, wdw_2=10)\n",
    "da_edge[0].frames.plot(vmin=-10, vmax=10, cmap=\"RdBu_r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2aa02e",
   "metadata": {},
   "source": [
    "#### Projection\n",
    "Essential for velocity estimation is the projection part. If you have a proper ``CameraConfig`` in your video, which includes the intrinsic matrix and possible distortion coefficients of your camera lens, and ground control points that gives pairs of real-world 3D coordinates and coordinates as they are in your 2D objective, then you can reproject your data into real-world coordinates. For PIV typically, we recommend to reproject to a resolution of 2 to 5 cm (i.e. 0.02 to 0.05). 5 cm is in our expertise usually sufficient, but dependent on the scale of structure that you may wish to follow, you may have to change this into a lower resolution. Let's have a look at a reprojected image. Note that this can also be done in RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd82db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.688825Z",
     "iopub.status.busy": "2024-09-09T07:53:40.688490Z",
     "iopub.status.idle": "2024-09-09T07:53:40.701407Z",
     "shell.execute_reply": "2024-09-09T07:53:40.700885Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's retrieve rgb frames\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m da_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mvideo\u001b[49m\u001b[38;5;241m.\u001b[39mget_frames(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# now reproject\u001b[39;00m\n\u001b[1;32m      4\u001b[0m da_proj \u001b[38;5;241m=\u001b[39m da_rgb\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mproject()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "# let's retrieve rgb frames\n",
    "da_rgb = video.get_frames(method=\"rgb\")\n",
    "# now reproject\n",
    "da_proj = da_rgb.frames.project()\n",
    "# and plot!\n",
    "da_proj[0].frames.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2c0b2",
   "metadata": {},
   "source": [
    "The ``edge_detection`` filter can better be applied on already projected images, in order to ensure that the window sizes set for the filters are in the real spatial domain. The reprojection above is 0.01 meter in resolution, hence if you wish to reveal edges of structure of a few centimeters in size, the filter sizes should be in that range as well. An example with window sizes of 2 and 4 pixels (i.e. 2 and 4 centimeter) on projected grayscale images is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02edc5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.703442Z",
     "iopub.status.busy": "2024-09-09T07:53:40.703133Z",
     "iopub.status.idle": "2024-09-09T07:53:40.716262Z",
     "shell.execute_reply": "2024-09-09T07:53:40.715736Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# repeat the reprojection, but now with normalizes images, and with greyscale.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# let's retrieve rgb frames\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m da_grey \u001b[38;5;241m=\u001b[39m \u001b[43mvideo\u001b[49m\u001b[38;5;241m.\u001b[39mget_frames()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# normalize first. This can be done in the original objective\u001b[39;00m\n\u001b[1;32m      5\u001b[0m da_norm \u001b[38;5;241m=\u001b[39m da_grey\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mnormalize()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "# repeat the reprojection, but now with normalizes images, and with greyscale.\n",
    "# let's retrieve rgb frames\n",
    "da_grey = video.get_frames()\n",
    "# normalize first. This can be done in the original objective\n",
    "da_norm = da_grey.frames.normalize()\n",
    "# now reproject\n",
    "da_proj = da_norm.frames.project()\n",
    "# after reprojection, do edge detection\n",
    "da_edge = da_proj.frames.edge_detect(wdw_1=2, wdw_2=4)\n",
    "# and plot!\n",
    "da_edge[0].frames.plot(vmin=-10, vmax=10, cmap=\"RdBu_r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba70aad",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "Geographical awareness is a really important feature of the frames ``xr.DataArray`` objects. So far we performed plotting in a local projection. The figure above has units meters on both the x-axis and y-axis. But after projection, geographical projections are also stored if the user supplied a coordinate reference system in the ``CameraConfig``. Below you can see that new coordinate variables ``lon`` and ``lat`` are available on all project ``xr.DataArray`` variables. The ``mode`` option with value ``geographical`` is used to decide which mode to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22eba2dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.718346Z",
     "iopub.status.busy": "2024-09-09T07:53:40.718039Z",
     "iopub.status.idle": "2024-09-09T07:53:40.729299Z",
     "shell.execute_reply": "2024-09-09T07:53:40.728817Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'da_edge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mda_edge\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'da_edge' is not defined"
     ]
    }
   ],
   "source": [
    "da_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ac192",
   "metadata": {},
   "source": [
    "These new coordinate variables can be interpreted by the ``.frames.plot`` functionality to also make a geographical plot, which can be combined with other geographical data you may have,m such as background maps, OpenStreetMap, satellite background and so on. The ``cartopy`` package is used to provide this geographical awareness to your plots. We refer to [cartopy's documentation](https://scitools.org.uk/cartopy/docs/latest/) for more information. An example of geographical plots and the camera's objective plot are provided below. Note that the ``.frames.plot`` function always returns the mappable. This can then be used to retrieve the axes of the mappable for instance to add other things to that axes, or to add a legend or colorbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbab882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.731417Z",
     "iopub.status.busy": "2024-09-09T07:53:40.731098Z",
     "iopub.status.idle": "2024-09-09T07:53:40.746095Z",
     "shell.execute_reply": "2024-09-09T07:53:40.745613Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'da_edge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mccrs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# when using mode=\"geographical\" and not supplying an axes, the plot function will make a geogrpahical aware axes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# for you.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mda_edge\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mplot(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeographical\u001b[39m\u001b[38;5;124m\"\u001b[39m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRdBu_r\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tiles = cimgt.OSM()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tiles \u001b[38;5;241m=\u001b[39m cimgt\u001b[38;5;241m.\u001b[39mGoogleTiles(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msatellite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'da_edge' is not defined"
     ]
    }
   ],
   "source": [
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.crs as ccrs\n",
    "# when using mode=\"geographical\" and not supplying an axes, the plot function will make a geogrpahical aware axes\n",
    "# for you.\n",
    "p = da_edge[0].frames.plot(mode=\"geographical\", vmin=-10, vmax=10, cmap=\"RdBu_r\")\n",
    "# tiles = cimgt.OSM()\n",
    "tiles = cimgt.GoogleTiles(style=\"satellite\")\n",
    "# add a google satellite image using cartopy functionality\n",
    "p.axes.add_image(tiles, 19)\n",
    "# zoom out a little bit so that we can actually see a bit\n",
    "p.axes.set_extent([\n",
    "    da_edge.lon.min() - 0.0005,\n",
    "    da_edge.lon.max() + 0.0005,\n",
    "    da_edge.lat.min() - 0.0005,\n",
    "    da_edge.lat.max() + 0.0005],\n",
    "    crs=ccrs.PlateCarree()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f6771",
   "metadata": {},
   "source": [
    "#### Animations\n",
    "You can store the results of your frames operations to a video in two ways:\n",
    "\n",
    "- ``.frames.to_ani``: stores an animation of the frames using your provided settings. These are passed to ``matplotlib.pyplot.imshow``. So all inputs to that function you are used to can be used.\n",
    "- ``.frames.to_video``: this will store the values in a video file using ``opencv``. It means that all values are scaled to between 0 and 255 and only greyscale video will be stored without any axis information. This is useful to quickly check the result of your operations.\n",
    "\n",
    "Below we give an example of how these functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df952552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T07:53:40.748126Z",
     "iopub.status.busy": "2024-09-09T07:53:40.747812Z",
     "iopub.status.idle": "2024-09-09T07:53:40.760677Z",
     "shell.execute_reply": "2024-09-09T07:53:40.760226Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'da_edge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we will only store the first 30 frames\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m da_sel \u001b[38;5;241m=\u001b[39m \u001b[43mda_edge\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m30\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# first with all color control\u001b[39;00m\n\u001b[1;32m      5\u001b[0m da_sel\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mto_ani(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_ani.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRdBu_r\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'da_edge' is not defined"
     ]
    }
   ],
   "source": [
    "# we will only store the first 30 frames\n",
    "da_sel = da_edge[0:30]\n",
    "\n",
    "# first with all color control\n",
    "da_sel.frames.to_ani(\"edge_ani.mp4\", vmin=-10, vmax=10, cmap=\"RdBu_r\")\n",
    "\n",
    "# now to a raw video without control over colors and axis\n",
    "# we want to maintain the min and max range, so have to explicitly use that\n",
    "da_scaled = np.maximum(np.minimum(da_sel, 10), -10)\n",
    "da_scaled.frames.to_video(\"edge_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16e41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
