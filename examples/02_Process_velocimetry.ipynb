{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5479fb0",
   "metadata": {},
   "source": [
    "## Analyze surface velocities of a video with velocimetry\n",
    "This notebook shows how to use a camera configuration, and a video to estimate velocities at the surface. \n",
    "It also demonstrates the important impact of filtering of spurious or noisy velocities on the end result. We go through the following steps:\n",
    "\n",
    "* Read a pre-defined camera configuration from file (we use the one prepared in notebook 1)\n",
    "* Open a video, and provide the predefined camera configuration to it.\n",
    "* Project frames to a planar projection\n",
    "* Estimate surface velocities with Particle Image Velocimetry\n",
    "* Filter raw velocities using several temporal and spatial filters\n",
    "* Plot results in the camera objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyorc\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bc15b",
   "metadata": {},
   "source": [
    "### load our camera configuration\n",
    "If you didn't do notebook 01, first go through that notebook to understand how a camera configuration is made.\n",
    "\n",
    "Below, the camera configuration is loaded back into memory, and used to open a video file. We only need a couple of seconds video, so we use frame 0 until frame 125 only. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f78a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cam_config = pyorc.load_camera_config(\"ngwerere/ngwerere.json\")\n",
    "video_file = \"ngwerere/ngwerere_20191103.mp4\"\n",
    "video = pyorc.Video(video_file, camera_config=cam_config, start_frame=0, end_frame=125)\n",
    "video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785ac1c",
   "metadata": {},
   "source": [
    "### extracting gray scaled frames\n",
    "You can see the video holds information about the video itself (filename, fps, and so on) but also the camera configuration supplied to it. We can now extract the frames. Without any arguments, the frames are automatically grayscaled and all frames are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = video.get_frames()\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c8d5d",
   "metadata": {},
   "source": [
    "The frames object is really a `xarray.DataFrame` object, with some additional functionalities under the method `.frames`. The beauty of our API is that it also uses lazy dask arrays to prevent very lengthy runs that then result in gibberish because of a small mistake along the way. We can see the shape and datatype of the end result, without actually computing everything, until we request a sample. Let's have a look at only the first frame with the plotting functionalities. If you want to use the default plot functionalities of `xarray` simply replace the line below by:\n",
    "```\n",
    "da[0].plot(cmap=\"gray\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69675693",
   "metadata": {},
   "outputs": [],
   "source": [
    "da[0].frames.plot(cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e74d7",
   "metadata": {},
   "source": [
    "### normalize to add contrast\n",
    "the `.frames` methods hold functionalities to improve the contrast of the image. A very good step is to remove the average of a larger set of frames from the frames itself, so that only strongly contrasting patterns from the background are left over. These are better traceable. We do this with the `.normalize` method. By default, 15 samples are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_norm = da.frames.normalize()\n",
    "da_norm[0].frames.plot(cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e1db0",
   "metadata": {},
   "source": [
    "A lot more contrast is visible now. We can now project the frames to an orthoprojected plane. The camera configuration, which is part of the `Video` object is used under the hood to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16, 9))\n",
    "da_norm_proj = da_norm.frames.project()\n",
    "da_norm_proj[0].frames.plot(cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f401a",
   "metadata": {},
   "source": [
    "You can see that the frames now also have x and y coordinates. These are in fact geographically aware, because we measured control points in real world coordinates and added a coordinate reference system to the `CameraConfig` object (see notebook 01). The `DataArray` therefore also contains coordinate grids for `lon` and `lat` for longitudes and latitudes. Hence we can also go through this entire pipeline with an RGB image and plot this in the real world by adding `mode=\"geographical\"` to the plotting functionalities. The grid is rotated so that its orientation always can follow the stream (in the local projection shown above, left is upstream, right downstream). \n",
    "Plotting of an rgb frame geographically is done as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frames again, but now with rgb\n",
    "da_rgb = video.get_frames(method=\"rgb\")\n",
    "# project the rgb frames, same as before\n",
    "da_rgb_proj = da_rgb.frames.project()\n",
    "# plot the first frame in geographical mode\n",
    "p = da_rgb_proj[0].frames.plot(mode=\"geographical\")\n",
    "\n",
    "# for fun, let's also add a satellite background from cartopy\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.crs as ccrs\n",
    "tiles = cimgt.GoogleTiles(style=\"satellite\")\n",
    "p.axes.add_image(tiles, 19)\n",
    "# zoom out a little bit so that we can actually see a bit\n",
    "p.axes.set_extent([\n",
    "    da_rgb_proj.lon.min() - 0.0001,\n",
    "    da_rgb_proj.lon.max() + 0.0001,\n",
    "    da_rgb_proj.lat.min() - 0.0001,\n",
    "    da_rgb_proj.lat.max() + 0.0001],\n",
    "    crs=ccrs.PlateCarree()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d832fc0",
   "metadata": {},
   "source": [
    "### Velocimetry estimates\n",
    "Now that we have real-world projected frames, with contrast enhanced, let's do some velocimetry! For Particle Image Velocimetry, this is as simple as calling the `.get_piv` method on the frames. Again a lazy result is returned really fast. If you want to do the computations, you can either extract a single frame, or (as below) store the result in a nice NetCDF file. Note that this file can be loaded back into memory with the `xarray` API without any additional fuss. We use a delayed method for storing, just to see a progress bar. If you are not interested in that, you can also replace the last 3 lines by:\n",
    "```\n",
    "piv.to_netcdf(\"ngwerere_piv.nc\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449237d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "piv = da_norm_proj.frames.get_piv()\n",
    "delayed_obj = piv.to_netcdf(\"ngwerere_piv.nc\", compute=False)\n",
    "with ProgressBar():\n",
    "    results = delayed_obj.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2390213",
   "metadata": {},
   "source": [
    "### Beautiful additions to your art gallery\n",
    "Of course now we want to do some plotting and filtering, for that, please go to the next notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
